{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Y9PLRL3bxu"
      },
      "source": [
        "*This is a Google Collab notebook*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3d-gR5w3lik",
        "outputId": "ad759a69-958e-4158-9034-eb0a1c64e215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0w6ACAkGpPJ"
      },
      "source": [
        "# **Unzipping** the datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKYWMOeEDL4B"
      },
      "outputs": [],
      "source": [
        "#Extracting diffusion dataset from Drive\n",
        "import zipfile\n",
        "zip_file_path = '/content/drive/MyDrive/Copy of diffusion_datasets.zip'\n",
        "extracted_dir = '/content/diffusion'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSecEnOlF1Yu"
      },
      "outputs": [],
      "source": [
        "#Extracting GAN dataset from Drive\n",
        "import zipfile\n",
        "zip_file_path = '/content/drive/MyDrive/Copy of CNN_synth_testset.zip'\n",
        "extracted_dir = '/content/gan'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnX3CpGlU22q",
        "outputId": "345670bb-3cd1-41ff-9cb8-85cd1718d4a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-e17651aa7b01>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "#Checking availability of gpu\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddsYs6mR4S1"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DFnbu6R3IVCq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "#Tensorflow Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_F7DPQJaZha"
      },
      "source": [
        "# **Level-1** of the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImC43ypASNfu"
      },
      "source": [
        "Gathering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2CiZ-k5IFql"
      },
      "outputs": [],
      "source": [
        "fake_images=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/dalle/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Reading image nd convert to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    fake_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_2git8QIin-"
      },
      "outputs": [],
      "source": [
        "real_images=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/laion/0_real\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        real_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtY7qQoBIxIs"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "labels=[]  # 0-Real 1-Fake\n",
        "\n",
        "#Data with both real and fake images\n",
        "data.extend(real_images)\n",
        "data.extend(fake_images[:988])\n",
        "\n",
        "#Labels for the data 0 and 1's\n",
        "l1=[0]*988\n",
        "l2=[1]*988\n",
        "labels.extend(l1)\n",
        "labels.extend(l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P_dlLahJBdf"
      },
      "outputs": [],
      "source": [
        "#Splitting training data into training and validation using sklearn\n",
        "#validation=20%\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjVmmoshSY0g"
      },
      "source": [
        "# Inserting data into training and validation folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igOoqkctSnAc"
      },
      "outputs": [],
      "source": [
        "# Function to add data in folder according to its class\n",
        "def create_fld(images, labels, dest):\n",
        "    i=0\n",
        "    for image, label in zip(images, labels):\n",
        "        i=i+1\n",
        "        class_dir=os.path.join(dest, str(label))\n",
        "        os.makedirs(class_dir,exist_ok=True) #Creating a directory or each class and when created it doesnt create again because exist=True condition passed\n",
        "        os.chdir(class_dir)\n",
        "        output_file=str(i)+\".jpg\"\n",
        "        cv2.imwrite(output_file,image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnO9Zgi8JLmL"
      },
      "outputs": [],
      "source": [
        "train_directory = \"/content/train_l1\"\n",
        "val_directory =  \"/content/validation_l1\"\n",
        "\n",
        "#Creating empty to directory for Training and Validation\n",
        "os.makedirs(train_directory)\n",
        "os.makedirs(val_directory)\n",
        "\n",
        "create_fld(X_train, y_train, train_directory)\n",
        "create_fld(X_val, y_val, val_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJIXvJYGTF4o"
      },
      "source": [
        "# **Creating Structure of the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Nb4z-l3byI"
      },
      "source": [
        "**1) Preprocessing of Data:**\n",
        "\n",
        "- Gaussian Blur.\n",
        "- Rescaling, rotation, flipping.\n",
        "- Preparing Training and Validation Generator.\n",
        "\n",
        "**2) Building Model:**\n",
        "\n",
        "- Importing weights from ResNet50 model (Pre-Trained Model).\n",
        "- **Fine Tuning:**\n",
        "  - Freezing first 150 layers (Neural network won't train back first 150 layers).\n",
        "  - Unfreezing all other layers (Weights get updated for all layers after the 150th layer).\n",
        "- Used Early Stopping to stop the training process when no significant improvement in loss is observed and retaining the best weights in each step.\n",
        "- Loss function: Binary Cross Entropy.\n",
        "\n",
        "**3) Model Structure:**\n",
        "\n",
        "- `x = base_model(inputs, training=False)`: Base_model is a pre-trained convolutional neural network (CNN) ResNet-50, used as a feature extractor.\n",
        "- `x = GlobalAveragePooling2D()(x)`: After extracting features from the base model, a global average pooling layer is applied. Global Average Pooling 2D computes the average value of each feature map across the entire spatial dimensions. This reduces the spatial dimensions to 1x1, effectively summarizing the information in each feature map.\n",
        "- `x = Dense(1024, activation='relu')(x)`: This is a fully connected (dense) layer with 1024 units and ReLU (Rectified Linear Unit) activation function. The output of the global average pooling is connected to this dense layer, introducing non-linearity and allowing the network to learn complex patterns.\n",
        "- `x = tf.keras.layers.Dropout(0.2)(x)`: Dropout is a regularization technique that helps prevent overfitting. It randomly sets a fraction of input units to zero at each update during training, which helps prevent the network from relying too much on any specific set of neurons. In this case, 20% of the units are dropped out (set to zero).\n",
        "- `outputs = Dense(1, activation='sigmoid')(x)`: The final layer is a dense layer with a single unit and a sigmoid activation function. The output is a probability between 0 and 1.\n",
        "  - 0: Real Image\n",
        "  - 1: Fake Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLpFCJSB3byI"
      },
      "source": [
        "**Hyperparameters:**\n",
        "\n",
        "- Learning Rate: 0.00001\n",
        "- Epochs: 40\n",
        "- Momentum: 0.9\n",
        "- Dropout Layers: 0.2\n",
        "- Optimizer: SGD (Stochastic Gradient Descent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T9klqW1wIK1"
      },
      "outputs": [],
      "source": [
        "def model_l1(train_dir,validation_dir):\n",
        "    #Applying gaussian blur to each image\n",
        "    def apply_gaussian_blur(image):\n",
        "        sigma = 1.0\n",
        "        return cv2.GaussianBlur(image, (0, 0), sigma)\n",
        "\n",
        "    #Data Augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        preprocessing_function=apply_gaussian_blur\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    #Training Data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    #Validation Data\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(256, 256, 3)) #Input shape\n",
        "\n",
        "    #Loading weights from Resnet50\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "    #Fine-Tuning\n",
        "    fine_tune_at = 150\n",
        "\n",
        "    #Freezing first 150 layers\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    #Unfreezing all layers ater 150\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    #Adding layers to the model\n",
        "    x = base_model(inputs, training=False)  #Resnet50 weights\n",
        "    x = GlobalAveragePooling2D()(x)         #Dimension reduction layer\n",
        "    x = Dense(1024, activation='relu')(x)   #FCNN layer\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)     #Droping out few weights to remove dependency of model on any particular weight\n",
        "    outputs = Dense(1, activation='sigmoid')(x) #Dense layer(o//p) layer 0 and 1\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    #Early stopping stops epochs if mofel not imporving in each step\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "    #compiling the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.00001, momentum=0.9),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #Fitting the model nd training for 40 epochs\n",
        "    history=model.fit(train_generator, epochs=40,\n",
        "              validation_data=validation_generator,\n",
        "              callbacks=[early_stopping_callback])\n",
        "\n",
        "    #Returning model nd history\n",
        "    return model,history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blA-ssOGHfcm"
      },
      "source": [
        "# **Training Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luaANyATVTUc",
        "outputId": "afdfb414-e5d8-43eb-d50e-6b05cbbcfba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1580 images belonging to 2 classes.\n",
            "Found 396 images belonging to 2 classes.\n",
            "Number of layers in the base model:  175\n",
            "Epoch 1/40\n",
            "53/53 [==============================] - 41s 677ms/step - loss: 0.7140 - accuracy: 0.5222 - val_loss: 0.6940 - val_accuracy: 0.5101\n",
            "Epoch 2/40\n",
            "53/53 [==============================] - 31s 582ms/step - loss: 0.7197 - accuracy: 0.4956 - val_loss: 0.6900 - val_accuracy: 0.5455\n",
            "Epoch 3/40\n",
            "53/53 [==============================] - 31s 587ms/step - loss: 0.7079 - accuracy: 0.5190 - val_loss: 0.6892 - val_accuracy: 0.5631\n",
            "Epoch 4/40\n",
            "53/53 [==============================] - 32s 610ms/step - loss: 0.7081 - accuracy: 0.5089 - val_loss: 0.6881 - val_accuracy: 0.5707\n",
            "Epoch 5/40\n",
            "53/53 [==============================] - 32s 599ms/step - loss: 0.7091 - accuracy: 0.5203 - val_loss: 0.6867 - val_accuracy: 0.5581\n",
            "Epoch 6/40\n",
            "53/53 [==============================] - 31s 585ms/step - loss: 0.7126 - accuracy: 0.4880 - val_loss: 0.6871 - val_accuracy: 0.5631\n",
            "Epoch 7/40\n",
            "53/53 [==============================] - 32s 598ms/step - loss: 0.7112 - accuracy: 0.4949 - val_loss: 0.6860 - val_accuracy: 0.5682\n",
            "Epoch 8/40\n",
            "53/53 [==============================] - 31s 584ms/step - loss: 0.6980 - accuracy: 0.5215 - val_loss: 0.6852 - val_accuracy: 0.5732\n",
            "Epoch 9/40\n",
            "53/53 [==============================] - 31s 591ms/step - loss: 0.7129 - accuracy: 0.4949 - val_loss: 0.6844 - val_accuracy: 0.5707\n",
            "Epoch 10/40\n",
            "53/53 [==============================] - 31s 589ms/step - loss: 0.7084 - accuracy: 0.4975 - val_loss: 0.6833 - val_accuracy: 0.5808\n",
            "Epoch 11/40\n",
            "53/53 [==============================] - 32s 603ms/step - loss: 0.7040 - accuracy: 0.5228 - val_loss: 0.6832 - val_accuracy: 0.5732\n",
            "Epoch 12/40\n",
            "53/53 [==============================] - 31s 580ms/step - loss: 0.7000 - accuracy: 0.5196 - val_loss: 0.6828 - val_accuracy: 0.5783\n",
            "Epoch 13/40\n",
            "53/53 [==============================] - 31s 591ms/step - loss: 0.6982 - accuracy: 0.5272 - val_loss: 0.6862 - val_accuracy: 0.5429\n",
            "Epoch 14/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.7003 - accuracy: 0.5278 - val_loss: 0.6816 - val_accuracy: 0.5808\n",
            "Epoch 15/40\n",
            "53/53 [==============================] - 32s 613ms/step - loss: 0.7013 - accuracy: 0.5342 - val_loss: 0.6810 - val_accuracy: 0.5884\n",
            "Epoch 16/40\n",
            "53/53 [==============================] - 32s 597ms/step - loss: 0.7005 - accuracy: 0.5215 - val_loss: 0.6855 - val_accuracy: 0.5455\n",
            "Epoch 17/40\n",
            "53/53 [==============================] - 33s 613ms/step - loss: 0.6993 - accuracy: 0.5291 - val_loss: 0.6820 - val_accuracy: 0.5934\n",
            "Epoch 18/40\n",
            "53/53 [==============================] - 31s 584ms/step - loss: 0.7066 - accuracy: 0.5266 - val_loss: 0.6808 - val_accuracy: 0.5859\n",
            "Epoch 19/40\n",
            "53/53 [==============================] - 31s 593ms/step - loss: 0.6871 - accuracy: 0.5462 - val_loss: 0.6790 - val_accuracy: 0.5934\n",
            "Epoch 20/40\n",
            "53/53 [==============================] - 32s 600ms/step - loss: 0.6912 - accuracy: 0.5405 - val_loss: 0.6764 - val_accuracy: 0.5884\n",
            "Epoch 21/40\n",
            "53/53 [==============================] - 31s 588ms/step - loss: 0.6889 - accuracy: 0.5386 - val_loss: 0.6815 - val_accuracy: 0.5808\n",
            "Epoch 22/40\n",
            "53/53 [==============================] - 31s 589ms/step - loss: 0.6963 - accuracy: 0.5348 - val_loss: 0.6764 - val_accuracy: 0.5960\n",
            "Epoch 23/40\n",
            "53/53 [==============================] - 31s 582ms/step - loss: 0.6919 - accuracy: 0.5462 - val_loss: 0.6775 - val_accuracy: 0.5934\n",
            "Epoch 24/40\n",
            "53/53 [==============================] - 32s 603ms/step - loss: 0.6867 - accuracy: 0.5392 - val_loss: 0.6762 - val_accuracy: 0.5934\n",
            "Epoch 25/40\n",
            "53/53 [==============================] - 33s 628ms/step - loss: 0.6816 - accuracy: 0.5614 - val_loss: 0.6802 - val_accuracy: 0.5783\n",
            "Epoch 26/40\n",
            "53/53 [==============================] - 32s 601ms/step - loss: 0.6880 - accuracy: 0.5715 - val_loss: 0.6773 - val_accuracy: 0.5859\n",
            "Epoch 27/40\n",
            "53/53 [==============================] - 33s 614ms/step - loss: 0.6888 - accuracy: 0.5513 - val_loss: 0.6753 - val_accuracy: 0.6010\n",
            "Epoch 28/40\n",
            "53/53 [==============================] - 31s 594ms/step - loss: 0.6898 - accuracy: 0.5437 - val_loss: 0.6792 - val_accuracy: 0.5859\n",
            "Epoch 29/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6797 - accuracy: 0.5772 - val_loss: 0.6732 - val_accuracy: 0.5985\n",
            "Epoch 30/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.6884 - accuracy: 0.5443 - val_loss: 0.6757 - val_accuracy: 0.5934\n",
            "Epoch 31/40\n",
            "53/53 [==============================] - 31s 587ms/step - loss: 0.6910 - accuracy: 0.5399 - val_loss: 0.6738 - val_accuracy: 0.6061\n",
            "Epoch 32/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.6783 - accuracy: 0.5665 - val_loss: 0.6705 - val_accuracy: 0.6061\n",
            "Epoch 33/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6808 - accuracy: 0.5608 - val_loss: 0.6735 - val_accuracy: 0.6111\n",
            "Epoch 34/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6804 - accuracy: 0.5608 - val_loss: 0.6668 - val_accuracy: 0.6086\n",
            "Epoch 35/40\n",
            "53/53 [==============================] - 33s 614ms/step - loss: 0.6811 - accuracy: 0.5595 - val_loss: 0.6710 - val_accuracy: 0.6061\n",
            "Epoch 36/40\n",
            "53/53 [==============================] - 31s 579ms/step - loss: 0.6830 - accuracy: 0.5589 - val_loss: 0.6692 - val_accuracy: 0.6111\n",
            "Epoch 37/40\n",
            "53/53 [==============================] - 31s 585ms/step - loss: 0.6775 - accuracy: 0.5601 - val_loss: 0.6712 - val_accuracy: 0.6086\n",
            "Epoch 38/40\n",
            "53/53 [==============================] - 31s 590ms/step - loss: 0.6795 - accuracy: 0.5709 - val_loss: 0.6645 - val_accuracy: 0.6010\n",
            "Epoch 39/40\n",
            "53/53 [==============================] - 32s 604ms/step - loss: 0.6798 - accuracy: 0.5608 - val_loss: 0.6664 - val_accuracy: 0.6162\n",
            "Epoch 40/40\n",
            "53/53 [==============================] - 30s 572ms/step - loss: 0.6617 - accuracy: 0.6063 - val_loss: 0.6650 - val_accuracy: 0.6162\n"
          ]
        }
      ],
      "source": [
        "t=\"/content/train_l1\"\n",
        "v=\"/content/validation_l1\"\n",
        "\n",
        "#Calling the above function\n",
        "mod_l1,hist_l1=model_l1(t,v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHg6PnC6TN7o"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrP6jbaQcV34",
        "outputId": "cba60f36-ba5f-4b88-b86c-540d6b5ff0d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "mod_l1.save(\"/content/Level1.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBCEerjuazcB"
      },
      "source": [
        "# Testing level-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqf_M8I2Lo3Q"
      },
      "outputs": [],
      "source": [
        "#Testing on glide\n",
        "test_images=[]\n",
        "test_label=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34CGlTWO3Fv2"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/glide_100_27/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6edVDh6c3pL7"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/glide_50_27/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9aSV7bM5Ck"
      },
      "outputs": [],
      "source": [
        "#Storing testing data into directory\n",
        "test_directory = \"/content/test\"\n",
        "os.makedirs(test_directory,exist_ok=True)\n",
        "create_fld(test_images,test_label, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g05spw5XNrGT",
        "outputId": "1a98d696-cc0d-44f4-9aa6-5f4d28496758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = \"/content/test\"\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSrVM4GDMEzZ",
        "outputId": "56e243bb-be16-4977-9f14-c7aaa8292a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 13s 127ms/step - loss: 0.6085 - accuracy: 0.8680\n",
            "Test Loss: 0.608487606048584\n",
            "Test Accuracy: 0.8679999709129333\n"
          ]
        }
      ],
      "source": [
        "#Evaluating our model\n",
        "results = mod_l1.evaluate(test_generator)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFcr19NQaBvq"
      },
      "source": [
        "# **Level-2** of the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3ElAjJT6fq"
      },
      "source": [
        "Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW30zPsjYKDw"
      },
      "outputs": [],
      "source": [
        "gan_images=[]\n",
        "path=\"/content/gan/whichfaceisreal/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Reading each image nd converting into np array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "\n",
        "    #If not a RGB image then discard it\n",
        "    if len(np.shape(img))==3:\n",
        "        new_size = (224, 224)\n",
        "        img = cv2.resize(img, new_size)  #Resizing image st all images are of same dimension\n",
        "        gan_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1VWE4KGsGIA",
        "outputId": "4ca6c7f1-e78b-4b5c-89d2-1e741d359e28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 224, 224, 3)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(gan_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOyf2C-HY5dV"
      },
      "outputs": [],
      "source": [
        "dm_images=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l:\n",
        "    #Reading each image nd converting into np array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "\n",
        "    #If not a RGB image then discard it\n",
        "    if len(np.shape(img))==3:\n",
        "      dm_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1w4n9fvtV1m",
        "outputId": "543abfcd-a9c5-4be9-eb64-a7d54b09d903"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 256, 256, 3)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(dm_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBZd1zaOZru0"
      },
      "outputs": [],
      "source": [
        "#Data containing both gan nd dm images\n",
        "data=[]\n",
        "labels=[]  # 0-Dm 1-Gan\n",
        "data.extend(dm_images)\n",
        "data.extend(gan_images)\n",
        "\n",
        "#Labels for both gan and dm images\n",
        "l1=[0]*len(dm_images)\n",
        "l2=[1]*len(gan_images)\n",
        "labels.extend(l1)\n",
        "labels.extend(l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2N6oTdmd_EE",
        "outputId": "b3e145dc-7445-4742-84bb-d6ca972a7e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total images: 2000 Dimension of each image: (256, 256, 3)\n",
            "(2000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Total images:\",len(data),\"Dimension of each image:\",np.shape(data[0]))\n",
        "print(np.shape(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyqxzb0eZqmN"
      },
      "outputs": [],
      "source": [
        "#Splitting data into train nd validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVgja7QAZVbv"
      },
      "outputs": [],
      "source": [
        "train_d = \"/content/train_l2\"\n",
        "val_d =  \"/content/validation_l2\"\n",
        "\n",
        "#Creating empty to directory for Training and Validation\n",
        "os.makedirs(train_d)\n",
        "os.makedirs(val_d)\n",
        "\n",
        "create_fld(X_train, y_train, train_d)\n",
        "create_fld(X_val, y_val, val_d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACp2AvWaINok"
      },
      "source": [
        "# **Creating Structure of Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMqHz_Ue3byb"
      },
      "source": [
        "**1) Preprocessing of Data:**\n",
        "\n",
        "- Gaussian Blur.\n",
        "- Rescaling, rotation, flipping.\n",
        "- Preparing Training and Validation Generator.\n",
        "\n",
        "**2) Building Model:**\n",
        "\n",
        "- Importing weights from ResNet50 model (Pre-Trained Model).\n",
        "- **Fine Tuning:**\n",
        "  - Freezing first 100 layers (Neural network won't train back first 100 layers).\n",
        "  - Unfreezing all other layers (Weights get updated for all layers after the 100th layer).\n",
        "- Used Early Stopping to stop the training process when no significant improvement in loss is observed and retaining the best weights in each step.\n",
        "- Loss function: Binary Cross Entropy.\n",
        "\n",
        "**3) Model Structure:**\n",
        "\n",
        "- `x = base_model(inputs, training=False)`: Base_model is a pre-trained convolutional neural network (CNN) ResNet-50, used as a feature extractor.\n",
        "- `x = GlobalAveragePooling2D()(x)`: After extracting features from the base model, a global average pooling layer is applied. Global Average Pooling 2D computes the average value of each feature map across the entire spatial dimensions. This reduces the spatial dimensions to 1x1, effectively summarizing the information in each feature map.\n",
        "- `x = Dense(1024, activation='relu')(x)`: This is a fully connected (dense) layer with 1024 units and ReLU (Rectified Linear Unit) activation function. The output of the global average pooling is connected to this dense layer, introducing non-linearity and allowing the network to learn complex patterns.\n",
        "- `x = tf.keras.layers.Dropout(0.2)(x)`: Dropout is a regularization technique that helps prevent overfitting. It randomly sets a fraction of input units to zero at each update during training, which helps prevent the network from relying too much on any specific set of neurons. In this case, 20% of the units are dropped out (set to zero).\n",
        "- `outputs = Dense(1, activation='sigmoid')(x)`: The final layer is a dense layer with a single unit and a sigmoid activation function. The output is a probability between 0 and 1.\n",
        "  - 0: Real Image\n",
        "  - 1: Fake Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNjRJ2R3byb"
      },
      "source": [
        "**Learning Rate Schedule: Exponential Decay**\n",
        "\n",
        "- **Initial Learning Rate:** \\(1 \\times 10^{-4}\\) (0.0001)\n",
        "  - The starting learning rate at the beginning of training.\n",
        "\n",
        "- **Decay Steps:** 10,000\n",
        "  - After every 10,000 steps, the learning rate will be updated.\n",
        "\n",
        "- **Decay Rate:** 0.9\n",
        "  - The rate at which the learning rate will decay, multiplied at each decay step.\n",
        "\n",
        "By employing this exponentially decaying learning rate schedule, the model takes larger steps initially and gradually decreases the learning rate as training progresses, aiding in faster convergence and fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkth0HL_3byb"
      },
      "source": [
        "**Hyperparameters:**\n",
        "\n",
        "- Epochs: 40\n",
        "- Momentum: 0.9\n",
        "- Dropout Layers: 0.2\n",
        "- Optimizer: SGD (Stochastic Gradient Descent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmI5Wn9OIIwp"
      },
      "outputs": [],
      "source": [
        "def model_l2(train_dir,validation_dir,tune,epoc):\n",
        "    #Applying gaussian blur to each image\n",
        "    def apply_gaussian_blur(image):\n",
        "        sigma = 1.0\n",
        "        return cv2.GaussianBlur(image, (0, 0), sigma)\n",
        "\n",
        "    #DATA Augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        preprocessing_function=apply_gaussian_blur\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    #Training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "    #VAlidation data\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "    inputs = tf.keras.Input(shape=(256, 256, 3)) #Input shape\n",
        "\n",
        "    #Loading weights from Resnet50\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "    #Fine-Tuning\n",
        "    fine_tune_at = tune\n",
        "\n",
        "    #Freezing first 100 layers\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    #Unfreezing all layers ater 100\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    #Adding layers to the model\n",
        "    x = base_model(inputs, training=False)  #Resnet50 weights\n",
        "    x = GlobalAveragePooling2D()(x)         #Dimension reduction layer\n",
        "    x = Dense(1024, activation='relu')(x)   #FCNN layer\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)     #Droping out few weights to remove dependency of model on any particular weight\n",
        "    outputs = Dense(1, activation='sigmoid')(x) #Dense layer(o//p) layer 0 and 1\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    #Learning Rate in terms of exponential Decay\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9\n",
        "    )\n",
        "    #Early stopping stops epochs if mofel not imporving in each step\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "    #compiling the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #Fitting the model nd training for 40 epochs\n",
        "    history=model.fit(train_generator, epochs=epoc,\n",
        "              validation_data=validation_generator,\n",
        "              callbacks=[early_stopping_callback])\n",
        "\n",
        "    #Returning model nd history\n",
        "    return model,history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKubA3rUTyNv"
      },
      "source": [
        "# **Training** phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ9erjgOZ7hh",
        "outputId": "f4aefdd5-7233-4360-9653-64efc261e1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Number of layers in the base model:  175\n",
            "Epoch 1/40\n",
            "54/54 [==============================] - 45s 612ms/step - loss: 0.6936 - accuracy: 0.5431 - val_loss: 0.6638 - val_accuracy: 0.5150\n",
            "Epoch 2/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.7032 - accuracy: 0.5169 - val_loss: 0.6831 - val_accuracy: 0.4950\n",
            "Epoch 3/40\n",
            "54/54 [==============================] - 33s 596ms/step - loss: 0.6690 - accuracy: 0.5838 - val_loss: 0.6101 - val_accuracy: 0.7200\n",
            "Epoch 4/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.6123 - accuracy: 0.6737 - val_loss: 0.6488 - val_accuracy: 0.5550\n",
            "Epoch 5/40\n",
            "54/54 [==============================] - 33s 608ms/step - loss: 0.5849 - accuracy: 0.6662 - val_loss: 0.6206 - val_accuracy: 0.8325\n",
            "Epoch 6/40\n",
            "54/54 [==============================] - 33s 611ms/step - loss: 0.5899 - accuracy: 0.6837 - val_loss: 0.6921 - val_accuracy: 0.4975\n",
            "Epoch 7/40\n",
            "54/54 [==============================] - 32s 600ms/step - loss: 0.6570 - accuracy: 0.6256 - val_loss: 0.5553 - val_accuracy: 0.8625\n",
            "Epoch 8/40\n",
            "54/54 [==============================] - 33s 602ms/step - loss: 0.4993 - accuracy: 0.7794 - val_loss: 0.3125 - val_accuracy: 0.9125\n",
            "Epoch 9/40\n",
            "54/54 [==============================] - 33s 614ms/step - loss: 0.4651 - accuracy: 0.7881 - val_loss: 0.2573 - val_accuracy: 0.9125\n",
            "Epoch 10/40\n",
            "54/54 [==============================] - 34s 614ms/step - loss: 0.2851 - accuracy: 0.8925 - val_loss: 0.2441 - val_accuracy: 0.9150\n",
            "Epoch 11/40\n",
            "54/54 [==============================] - 33s 609ms/step - loss: 0.2260 - accuracy: 0.9156 - val_loss: 0.1171 - val_accuracy: 0.9550\n",
            "Epoch 12/40\n",
            "54/54 [==============================] - 33s 617ms/step - loss: 0.2048 - accuracy: 0.9275 - val_loss: 0.0737 - val_accuracy: 0.9775\n",
            "Epoch 13/40\n",
            "54/54 [==============================] - 32s 588ms/step - loss: 0.0944 - accuracy: 0.9669 - val_loss: 0.0447 - val_accuracy: 0.9900\n",
            "Epoch 14/40\n",
            "54/54 [==============================] - 34s 613ms/step - loss: 0.1051 - accuracy: 0.9594 - val_loss: 0.1146 - val_accuracy: 0.9525\n",
            "Epoch 15/40\n",
            "54/54 [==============================] - 33s 600ms/step - loss: 0.1038 - accuracy: 0.9631 - val_loss: 0.0319 - val_accuracy: 0.9875\n",
            "Epoch 16/40\n",
            "54/54 [==============================] - 33s 604ms/step - loss: 0.0752 - accuracy: 0.9744 - val_loss: 0.0237 - val_accuracy: 0.9950\n",
            "Epoch 17/40\n",
            "54/54 [==============================] - 32s 589ms/step - loss: 0.0345 - accuracy: 0.9869 - val_loss: 0.0257 - val_accuracy: 0.9925\n",
            "Epoch 18/40\n",
            "54/54 [==============================] - 34s 624ms/step - loss: 0.0506 - accuracy: 0.9794 - val_loss: 0.0403 - val_accuracy: 0.9850\n",
            "Epoch 19/40\n",
            "54/54 [==============================] - 32s 596ms/step - loss: 0.0527 - accuracy: 0.9831 - val_loss: 0.0288 - val_accuracy: 0.9900\n",
            "Epoch 20/40\n",
            "54/54 [==============================] - 32s 598ms/step - loss: 0.0263 - accuracy: 0.9931 - val_loss: 0.0680 - val_accuracy: 0.9625\n",
            "Epoch 21/40\n",
            "54/54 [==============================] - 33s 612ms/step - loss: 0.0301 - accuracy: 0.9875 - val_loss: 0.0144 - val_accuracy: 0.9925\n",
            "Epoch 22/40\n",
            "54/54 [==============================] - 35s 634ms/step - loss: 0.0207 - accuracy: 0.9925 - val_loss: 0.0404 - val_accuracy: 0.9800\n",
            "Epoch 23/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.0238 - val_accuracy: 0.9950\n",
            "Epoch 24/40\n",
            "54/54 [==============================] - 33s 606ms/step - loss: 0.0110 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9975\n",
            "Epoch 25/40\n",
            "54/54 [==============================] - 32s 589ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0216 - val_accuracy: 0.9900\n",
            "Epoch 26/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.0081 - accuracy: 0.9981 - val_loss: 0.0097 - val_accuracy: 0.9925\n",
            "Epoch 27/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0281 - val_accuracy: 0.9875\n",
            "Epoch 28/40\n",
            "54/54 [==============================] - 33s 602ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 0.0118 - val_accuracy: 0.9950\n",
            "Epoch 29/40\n",
            "54/54 [==============================] - 32s 592ms/step - loss: 0.0099 - accuracy: 0.9956 - val_loss: 0.0103 - val_accuracy: 0.9925\n",
            "Epoch 30/40\n",
            "54/54 [==============================] - 34s 633ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.0102 - val_accuracy: 0.9975\n",
            "Epoch 31/40\n",
            "54/54 [==============================] - 32s 596ms/step - loss: 0.0085 - accuracy: 0.9950 - val_loss: 0.0229 - val_accuracy: 0.9900\n",
            "Epoch 32/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0222 - val_accuracy: 0.9875\n",
            "Epoch 33/40\n",
            "54/54 [==============================] - 32s 591ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.0099 - val_accuracy: 0.9950\n",
            "Epoch 34/40\n",
            "54/54 [==============================] - 32s 594ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0198 - val_accuracy: 0.9925\n",
            "Epoch 35/40\n",
            "54/54 [==============================] - 32s 593ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9950\n",
            "Epoch 36/40\n",
            "54/54 [==============================] - 34s 627ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9950\n",
            "Epoch 37/40\n",
            "54/54 [==============================] - 34s 621ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9950\n",
            "Epoch 38/40\n",
            "54/54 [==============================] - 33s 603ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0153 - val_accuracy: 0.9950\n",
            "Epoch 39/40\n",
            "54/54 [==============================] - 33s 618ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0131 - val_accuracy: 0.9925\n",
            "Epoch 40/40\n",
            "54/54 [==============================] - 33s 597ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9950\n"
          ]
        }
      ],
      "source": [
        "t=\"/content/train_l2\"\n",
        "v=\"/content/validation_l2\"\n",
        "#Training the model\n",
        "mod_l2,hist_l2=model_l2(t,v,100,40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3YKECopTuHO"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or6YM7VWf8We",
        "outputId": "de3dea92-1eba-4de4-eccd-2dcfc00c9287"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "mod_l2.save(\"/content/Level2_updated.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IycL-pZ_e2hB"
      },
      "source": [
        "# Testing Level-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dso9j11meyv-"
      },
      "outputs": [],
      "source": [
        "test_images=[]\n",
        "test_label=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/guided/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LMh5puZbHl5"
      },
      "outputs": [],
      "source": [
        "path=\"/content/gan/stargan/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-HDAbNVfekS"
      },
      "outputs": [],
      "source": [
        "test_directory = \"/content/test1\"\n",
        "os.makedirs(test_directory,exist_ok=True)\n",
        "create_fld(test_images,test_label, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgDuvHd_fm6Z",
        "outputId": "87deb351-2ffc-45d4-8ee4-798d9c7d7333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2999 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = \"/content/test1\"\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqOhNe_8fox3",
        "outputId": "2367f833-a274-4193-fe62-7d18550e0c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 12s 123ms/step - loss: 0.2682 - accuracy: 0.9300\n",
            "Test Loss: 0.2682065963745117\n",
            "Test Accuracy: 0.9299766421318054\n"
          ]
        }
      ],
      "source": [
        "#Testing thr model\n",
        "results = mod_l2.evaluate(test_generator)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gruJelDSDHGR"
      },
      "source": [
        "# **Level-3 of the Architecture(DM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbWT6t_yEhT5"
      },
      "source": [
        "Detecting to which class the fake image belongs to\n",
        "Classes: Dalle Glide LDM Guided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajCy9IRKFRzV"
      },
      "source": [
        "Gathering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMEca9nBDAbE",
        "outputId": "ffff1f1d-aa72-46cd-db06-3c0e35f80e3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 256, 256, 3)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dalle=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/dalle/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[:800]\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        dalle.append(img)\n",
        "np.shape(dalle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iesLatl5Dt4_",
        "outputId": "46cfb726-7bcf-4328-b3e6-b0eb857575d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 256, 256, 3)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glide=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[:800]\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        glide.append(img)\n",
        "np.shape(glide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhj342d5ELve",
        "outputId": "a171ee9c-4f31-4023-f0dd-0d6b864b211c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 256, 256, 3)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "guided=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/guided/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[:800]\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        guided.append(img)\n",
        "np.shape(guided)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35-RsxROEWk1",
        "outputId": "407a1891-8f26-4ec2-9233-654f5a8d24b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 256, 256, 3)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldm=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/ldm_100/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[:800]\n",
        "\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        ldm.append(img)\n",
        "np.shape(ldm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSGtWo2YExt6",
        "outputId": "8319e96e-28b2-4749-edfe-1ddd641e1c89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3200, 256, 256, 3)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Adding all images to data\n",
        "data=[]\n",
        "data.extend(dalle)\n",
        "data.extend(glide)\n",
        "data.extend(guided)\n",
        "data.extend(ldm)\n",
        "np.shape(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRtRy87nFB2d",
        "outputId": "ab2aa671-611a-41a5-bac5-8379f2a7fa48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3200,)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creating labels\n",
        "labels=[0]*800\n",
        "labels.extend([1]*800)\n",
        "labels.extend([2]*800)\n",
        "labels.extend([3]*800)\n",
        "np.shape(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjjdh1CCFrFr"
      },
      "source": [
        "Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5SDoErWFiY8"
      },
      "outputs": [],
      "source": [
        "#Splitting training data into training and validation using sklearn\n",
        "#validation=10%\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Z_-n6XFto9"
      },
      "source": [
        "Inserting data to folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm5n3fzDFV4-"
      },
      "outputs": [],
      "source": [
        "train_directory = \"/content/train_l3a\"\n",
        "val_directory =  \"/content/validation_l3a\"\n",
        "\n",
        "#Creating empty to directory for Training and Validation\n",
        "os.makedirs(train_directory)\n",
        "os.makedirs(val_directory)\n",
        "\n",
        "create_fld(X_train, y_train, train_directory)\n",
        "create_fld(X_val, y_val, val_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iShXdXBGDzl"
      },
      "source": [
        "# **Model Architecture for Level-3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "KrzU2UYFJGdE"
      },
      "outputs": [],
      "source": [
        "def model_l3a(train_dir,validation_dir,tune,epoc):\n",
        "    #DATA Augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    #Training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "    #VAlidation data\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "    inputs = tf.keras.Input(shape=(256, 256, 3)) #Input shape\n",
        "\n",
        "    #Loading weights from Resnet50\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "    #Fine-Tuning\n",
        "    fine_tune_at = tune\n",
        "\n",
        "    #Freezing first 100 layers\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    #Unfreezing all layers ater 100\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    #Adding layers to the model\n",
        "    x = base_model(inputs, training=False)  #Resnet50 weights\n",
        "    x = GlobalAveragePooling2D()(x)         #Dimension reduction layer\n",
        "    x = Dense(1024, activation='relu')(x)   #FCNN layer\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)     #Droping out few weights to remove dependency of model on any particular weight\n",
        "    outputs = Dense(4, activation='softmax')(x) #Dense layer(o//p) layer 0 and 1\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    #Learning Rate in terms of exponential Decay\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9\n",
        "    )\n",
        "    #Early stopping stops epochs if mofel not imporving in each step\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "    #compiling the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    #Fitting the model nd training for 40 epochs\n",
        "    history=model.fit(train_generator, epochs=epoc,\n",
        "              validation_data=validation_generator,\n",
        "              callbacks=[early_stopping_callback])\n",
        "\n",
        "    #Returning model nd history\n",
        "    return model,history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BbrwAVfGKhz"
      },
      "source": [
        "# Training Level-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwtNXfUIHi9c",
        "outputId": "11b84ce4-6676-44e0-afaf-7dfa61074783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2880 images belonging to 4 classes.\n",
            "Found 320 images belonging to 4 classes.\n",
            "Number of layers in the base model:  175\n",
            "Epoch 1/40\n",
            "96/96 [==============================] - 65s 588ms/step - loss: 1.3537 - accuracy: 0.3257 - val_loss: 1.2482 - val_accuracy: 0.4500\n",
            "Epoch 2/40\n",
            "96/96 [==============================] - 56s 587ms/step - loss: 1.2632 - accuracy: 0.3837 - val_loss: 1.2239 - val_accuracy: 0.4875\n",
            "Epoch 3/40\n",
            "96/96 [==============================] - 56s 581ms/step - loss: 1.2383 - accuracy: 0.4056 - val_loss: 1.3034 - val_accuracy: 0.3375\n",
            "Epoch 4/40\n",
            "96/96 [==============================] - 55s 573ms/step - loss: 1.2251 - accuracy: 0.4205 - val_loss: 1.2884 - val_accuracy: 0.4313\n",
            "Epoch 5/40\n",
            "96/96 [==============================] - 57s 593ms/step - loss: 1.2144 - accuracy: 0.4292 - val_loss: 1.1770 - val_accuracy: 0.3844\n",
            "Epoch 6/40\n",
            "96/96 [==============================] - 54s 559ms/step - loss: 1.2115 - accuracy: 0.4292 - val_loss: 1.1497 - val_accuracy: 0.4750\n",
            "Epoch 7/40\n",
            "96/96 [==============================] - 57s 593ms/step - loss: 1.1801 - accuracy: 0.4545 - val_loss: 1.1127 - val_accuracy: 0.4688\n",
            "Epoch 8/40\n",
            "96/96 [==============================] - 54s 563ms/step - loss: 1.1801 - accuracy: 0.4444 - val_loss: 1.1071 - val_accuracy: 0.5063\n",
            "Epoch 9/40\n",
            "96/96 [==============================] - 55s 572ms/step - loss: 1.1434 - accuracy: 0.4670 - val_loss: 1.0524 - val_accuracy: 0.5781\n",
            "Epoch 10/40\n",
            "96/96 [==============================] - 53s 548ms/step - loss: 1.1574 - accuracy: 0.4726 - val_loss: 1.0781 - val_accuracy: 0.5094\n",
            "Epoch 11/40\n",
            "96/96 [==============================] - 55s 570ms/step - loss: 1.1610 - accuracy: 0.4632 - val_loss: 1.0560 - val_accuracy: 0.5375\n",
            "Epoch 12/40\n",
            "96/96 [==============================] - 56s 582ms/step - loss: 1.1508 - accuracy: 0.4771 - val_loss: 1.0320 - val_accuracy: 0.5406\n",
            "Epoch 13/40\n",
            "96/96 [==============================] - 55s 575ms/step - loss: 1.1390 - accuracy: 0.4889 - val_loss: 1.0560 - val_accuracy: 0.5437\n",
            "Epoch 14/40\n",
            "96/96 [==============================] - 53s 555ms/step - loss: 1.1318 - accuracy: 0.4826 - val_loss: 1.0293 - val_accuracy: 0.5813\n",
            "Epoch 15/40\n",
            "96/96 [==============================] - 54s 565ms/step - loss: 1.1093 - accuracy: 0.5031 - val_loss: 1.0725 - val_accuracy: 0.4969\n",
            "Epoch 16/40\n",
            "96/96 [==============================] - 54s 555ms/step - loss: 1.1054 - accuracy: 0.5003 - val_loss: 1.0417 - val_accuracy: 0.5219\n",
            "Epoch 17/40\n",
            "96/96 [==============================] - 57s 586ms/step - loss: 1.1342 - accuracy: 0.4781 - val_loss: 1.1205 - val_accuracy: 0.5437\n",
            "Epoch 18/40\n",
            "96/96 [==============================] - 54s 558ms/step - loss: 1.1105 - accuracy: 0.5069 - val_loss: 1.0220 - val_accuracy: 0.6031\n",
            "Epoch 19/40\n",
            "96/96 [==============================] - 55s 571ms/step - loss: 1.0934 - accuracy: 0.5118 - val_loss: 1.0474 - val_accuracy: 0.5469\n",
            "Epoch 20/40\n",
            "96/96 [==============================] - 54s 563ms/step - loss: 1.1011 - accuracy: 0.5090 - val_loss: 1.1666 - val_accuracy: 0.4375\n",
            "Epoch 21/40\n",
            "96/96 [==============================] - 57s 590ms/step - loss: 1.1579 - accuracy: 0.4618 - val_loss: 1.0947 - val_accuracy: 0.4625\n",
            "Epoch 22/40\n",
            "96/96 [==============================] - 54s 558ms/step - loss: 1.0920 - accuracy: 0.5167 - val_loss: 0.9737 - val_accuracy: 0.5938\n",
            "Epoch 23/40\n",
            "96/96 [==============================] - 55s 573ms/step - loss: 1.0693 - accuracy: 0.5205 - val_loss: 0.9933 - val_accuracy: 0.6062\n",
            "Epoch 24/40\n",
            "96/96 [==============================] - 53s 551ms/step - loss: 1.0595 - accuracy: 0.5465 - val_loss: 1.0048 - val_accuracy: 0.5875\n",
            "Epoch 25/40\n",
            "96/96 [==============================] - 57s 589ms/step - loss: 1.0757 - accuracy: 0.5170 - val_loss: 1.0442 - val_accuracy: 0.5312\n",
            "Epoch 26/40\n",
            "96/96 [==============================] - 56s 588ms/step - loss: 1.0610 - accuracy: 0.5163 - val_loss: 1.0732 - val_accuracy: 0.5688\n",
            "Epoch 27/40\n",
            "96/96 [==============================] - 54s 559ms/step - loss: 1.0415 - accuracy: 0.5455 - val_loss: 1.0267 - val_accuracy: 0.5750\n",
            "Epoch 28/40\n",
            "96/96 [==============================] - 53s 553ms/step - loss: 1.0363 - accuracy: 0.5441 - val_loss: 0.9671 - val_accuracy: 0.6219\n",
            "Epoch 29/40\n",
            "96/96 [==============================] - 55s 566ms/step - loss: 1.0591 - accuracy: 0.5333 - val_loss: 1.0128 - val_accuracy: 0.5813\n",
            "Epoch 30/40\n",
            "96/96 [==============================] - 53s 553ms/step - loss: 1.0224 - accuracy: 0.5378 - val_loss: 1.0004 - val_accuracy: 0.6438\n",
            "Epoch 31/40\n",
            "96/96 [==============================] - 58s 603ms/step - loss: 1.0234 - accuracy: 0.5514 - val_loss: 1.0106 - val_accuracy: 0.5875\n",
            "Epoch 32/40\n",
            "96/96 [==============================] - 53s 552ms/step - loss: 1.0313 - accuracy: 0.5403 - val_loss: 1.0103 - val_accuracy: 0.5906\n",
            "Epoch 33/40\n",
            "96/96 [==============================] - 55s 571ms/step - loss: 1.0036 - accuracy: 0.5757 - val_loss: 1.0144 - val_accuracy: 0.5969\n",
            "Epoch 34/40\n",
            "96/96 [==============================] - 53s 553ms/step - loss: 1.0158 - accuracy: 0.5601 - val_loss: 1.2025 - val_accuracy: 0.4094\n",
            "Epoch 35/40\n",
            "96/96 [==============================] - 57s 587ms/step - loss: 1.0187 - accuracy: 0.5490 - val_loss: 1.0292 - val_accuracy: 0.5469\n",
            "Epoch 36/40\n",
            "96/96 [==============================] - 54s 559ms/step - loss: 1.0355 - accuracy: 0.5316 - val_loss: 0.9819 - val_accuracy: 0.5938\n",
            "Epoch 37/40\n",
            "96/96 [==============================] - 56s 581ms/step - loss: 0.9894 - accuracy: 0.5753 - val_loss: 0.9884 - val_accuracy: 0.6156\n",
            "Epoch 38/40\n",
            "96/96 [==============================] - 55s 576ms/step - loss: 0.9834 - accuracy: 0.5663 - val_loss: 1.0676 - val_accuracy: 0.5531\n",
            "Epoch 39/40\n",
            "96/96 [==============================] - 56s 583ms/step - loss: 0.9824 - accuracy: 0.5674 - val_loss: 1.0447 - val_accuracy: 0.5719\n",
            "Epoch 40/40\n",
            "96/96 [==============================] - 55s 573ms/step - loss: 0.9846 - accuracy: 0.5767 - val_loss: 0.9794 - val_accuracy: 0.5781\n"
          ]
        }
      ],
      "source": [
        "t=\"/content/train_l3a\"\n",
        "v=\"/content/validation_l3a\"\n",
        "#Training the model\n",
        "mod_l3a,hist_l3a=model_l3a(t,v,100,40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBgClo_qH3zQ"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK_SiV3xHubv"
      },
      "outputs": [],
      "source": [
        "mod_l3a.save(\"/content/Level3a.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi38DK1fF-in"
      },
      "source": [
        "# **Testing Level-3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHyHGwtuFx5Z"
      },
      "outputs": [],
      "source": [
        "test_images=[]\n",
        "test_label=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/dalle/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[800:]\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqlpZNJ5Gd4h"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[800:]\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNgj3w7hGpYY"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/guided/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[800:]\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ugcm-hgGrDE"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/ldm_100/1_fake\"\n",
        "l=os.listdir(path) #List containing path of all images in this folder\n",
        "l.sort()\n",
        "l1=l[800:]\n",
        "#Iterating into each image\n",
        "for i in l1:\n",
        "    #Opening the image nd converting to numpy array\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    #If not a RGB image, then discarding it\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "OQM1FMkKG_8w"
      },
      "outputs": [],
      "source": [
        "test_directory = \"/content/test2\"\n",
        "os.makedirs(test_directory,exist_ok=True)\n",
        "create_fld(test_images,test_label, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-EFa5SNHMKU",
        "outputId": "94d43c1d-b2df-4661-e3de-267f440d6dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = \"/content/test2\"\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3IpCZbGHOka",
        "outputId": "9403c02f-a951-47e8-dfca-463971af3217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27/27 [==============================] - 3s 123ms/step - loss: 0.9937 - accuracy: 0.5500\n",
            "Test Loss: 0.9936790466308594\n",
            "Test Accuracy: 0.550000011920929\n"
          ]
        }
      ],
      "source": [
        "#Testing thr model\n",
        "results = mod_l3a.evaluate(test_generator)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
