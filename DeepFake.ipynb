{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This is a Google Collab notebook*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAeLf80KGg0X"
      },
      "source": [
        "# **Downloading** data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "IKpTMKg_h8sV",
        "outputId": "d675a859-2e1a-4dac-ddc7-20959b21b6cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LPu1TAYQqCTyugZcmuxU6JH8LI5gezw6\n",
            "To: /content/downloaded_file.zip\n",
            "100%|██████████| 20.1G/20.1G [03:57<00:00, 84.6MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'downloaded_file.zip'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "file_url = \"https://drive.google.com/uc?id=1LPu1TAYQqCTyugZcmuxU6JH8LI5gezw6\"\n",
        "output_path = 'downloaded_file.zip'\n",
        "gdown.download(file_url, output_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-xHxE9uJCxPX",
        "outputId": "3b57152e-ab27-41ec-b2be-98be8b2e584d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MWo9i1TCYMYxd3DX7RK2ttVnmwuJPDET\n",
            "To: /content/dms.zip\n",
            "100%|██████████| 918M/918M [00:10<00:00, 84.9MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dms.zip'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_url = \"https://drive.google.com/uc?id=1MWo9i1TCYMYxd3DX7RK2ttVnmwuJPDET\"\n",
        "output_path = 'dms.zip'\n",
        "gdown.download(file_url, output_path, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0w6ACAkGpPJ"
      },
      "source": [
        "# **Unzipping** the downloaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lKYWMOeEDL4B"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/dms.zip'\n",
        "extracted_dir = '/content/diffusion'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bSecEnOlF1Yu"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_file_path = '/content/downloaded_file.zip'\n",
        "extracted_dir = '/content/gan'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnX3CpGlU22q",
        "outputId": "8e18849b-d907-49b3-fee1-538386bd8277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-96e2f6ef306a>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddsYs6mR4S1"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DFnbu6R3IVCq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_F7DPQJaZha"
      },
      "source": [
        "# **Level-1** of the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImC43ypASNfu"
      },
      "source": [
        "Gathering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A2CiZ-k5IFql"
      },
      "outputs": [],
      "source": [
        "fake_images=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/dalle/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    fake_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j_2git8QIin-"
      },
      "outputs": [],
      "source": [
        "real_images=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/laion/0_real\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if(np.shape(img)==(256,256,3)):\n",
        "        real_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OtY7qQoBIxIs"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "labels=[]  # 0-Real 1-Fake\n",
        "data.extend(real_images)\n",
        "data.extend(fake_images[:988])\n",
        "l1=[0]*988\n",
        "l2=[1]*988\n",
        "labels.extend(l1)\n",
        "labels.extend(l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3P_dlLahJBdf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjVmmoshSY0g"
      },
      "source": [
        "# Inserting data into training and validation folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "igOoqkctSnAc"
      },
      "outputs": [],
      "source": [
        "# Function to add data in folder according to its class\n",
        "def copy_images_to_directory(images, labels, dest):\n",
        "    i=0\n",
        "    for image, label in zip(images, labels):\n",
        "        i=i+1\n",
        "        class_directory = os.path.join(dest, str(label))\n",
        "        os.makedirs(class_directory,exist_ok=True)\n",
        "        os.chdir(class_directory)\n",
        "        output_file=str(i)+\".jpg\"\n",
        "        cv2.imwrite(output_file,image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YnO9Zgi8JLmL"
      },
      "outputs": [],
      "source": [
        "train_directory = \"/content/train_l1\"\n",
        "val_directory =  \"/content/validation_l1\"\n",
        "os.makedirs(train_directory)\n",
        "os.makedirs(val_directory)\n",
        "\n",
        "copy_images_to_directory(X_train, y_train, train_directory)\n",
        "copy_images_to_directory(X_val, y_val, val_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJIXvJYGTF4o"
      },
      "source": [
        "# **Creating Structure of the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1) Preprocessing of Data:**\n",
        "\n",
        "- Gaussian Blur.\n",
        "- Rescaling, rotation, flipping.\n",
        "- Preparing Training and Validation Generator.\n",
        "\n",
        "**2) Building Model:**\n",
        "\n",
        "- Importing weights from ResNet50 model (Pre-Trained Model).\n",
        "- **Fine Tuning:**\n",
        "  - Freezing first 150 layers (Neural network won't train back first 150 layers).\n",
        "  - Unfreezing all other layers (Weights get updated for all layers after the 150th layer).\n",
        "- Used Early Stopping to stop the training process when no significant improvement in loss is observed and retaining the best weights in each step.\n",
        "- Loss function: Binary Cross Entropy.\n",
        "\n",
        "**3) Model Structure:**\n",
        "\n",
        "- `x = base_model(inputs, training=False)`: Base_model is a pre-trained convolutional neural network (CNN) ResNet-50, used as a feature extractor.\n",
        "- `x = GlobalAveragePooling2D()(x)`: After extracting features from the base model, a global average pooling layer is applied. Global Average Pooling 2D computes the average value of each feature map across the entire spatial dimensions. This reduces the spatial dimensions to 1x1, effectively summarizing the information in each feature map.\n",
        "- `x = Dense(1024, activation='relu')(x)`: This is a fully connected (dense) layer with 1024 units and ReLU (Rectified Linear Unit) activation function. The output of the global average pooling is connected to this dense layer, introducing non-linearity and allowing the network to learn complex patterns.\n",
        "- `x = tf.keras.layers.Dropout(0.2)(x)`: Dropout is a regularization technique that helps prevent overfitting. It randomly sets a fraction of input units to zero at each update during training, which helps prevent the network from relying too much on any specific set of neurons. In this case, 20% of the units are dropped out (set to zero).\n",
        "- `outputs = Dense(1, activation='sigmoid')(x)`: The final layer is a dense layer with a single unit and a sigmoid activation function. The output is a probability between 0 and 1.\n",
        "  - 0: Real Image\n",
        "  - 1: Fake Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hyperparameters:**\n",
        "\n",
        "- Learning Rate: 0.00001\n",
        "- Epochs: 40\n",
        "- Momentum: 0.9\n",
        "- Dropout Layers: 0.2\n",
        "- Optimizer: SGD (Stochastic Gradient Descent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7T9klqW1wIK1"
      },
      "outputs": [],
      "source": [
        "def model_l1(train_dir,validation_dir):\n",
        "    def apply_gaussian_blur(image):\n",
        "        sigma = 1.0\n",
        "        return cv2.GaussianBlur(image, (0, 0), sigma)\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        preprocessing_function=apply_gaussian_blur\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(256, 256, 3))\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "    fine_tune_at = 150\n",
        "\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.00001, momentum=0.9),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history=model.fit(train_generator, epochs=40,\n",
        "              validation_data=validation_generator,\n",
        "              callbacks=[early_stopping_callback])\n",
        "    return model,history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blA-ssOGHfcm"
      },
      "source": [
        "# **Training Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luaANyATVTUc",
        "outputId": "afdfb414-e5d8-43eb-d50e-6b05cbbcfba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1580 images belonging to 2 classes.\n",
            "Found 396 images belonging to 2 classes.\n",
            "Number of layers in the base model:  175\n",
            "Epoch 1/40\n",
            "53/53 [==============================] - 41s 677ms/step - loss: 0.7140 - accuracy: 0.5222 - val_loss: 0.6940 - val_accuracy: 0.5101\n",
            "Epoch 2/40\n",
            "53/53 [==============================] - 31s 582ms/step - loss: 0.7197 - accuracy: 0.4956 - val_loss: 0.6900 - val_accuracy: 0.5455\n",
            "Epoch 3/40\n",
            "53/53 [==============================] - 31s 587ms/step - loss: 0.7079 - accuracy: 0.5190 - val_loss: 0.6892 - val_accuracy: 0.5631\n",
            "Epoch 4/40\n",
            "53/53 [==============================] - 32s 610ms/step - loss: 0.7081 - accuracy: 0.5089 - val_loss: 0.6881 - val_accuracy: 0.5707\n",
            "Epoch 5/40\n",
            "53/53 [==============================] - 32s 599ms/step - loss: 0.7091 - accuracy: 0.5203 - val_loss: 0.6867 - val_accuracy: 0.5581\n",
            "Epoch 6/40\n",
            "53/53 [==============================] - 31s 585ms/step - loss: 0.7126 - accuracy: 0.4880 - val_loss: 0.6871 - val_accuracy: 0.5631\n",
            "Epoch 7/40\n",
            "53/53 [==============================] - 32s 598ms/step - loss: 0.7112 - accuracy: 0.4949 - val_loss: 0.6860 - val_accuracy: 0.5682\n",
            "Epoch 8/40\n",
            "53/53 [==============================] - 31s 584ms/step - loss: 0.6980 - accuracy: 0.5215 - val_loss: 0.6852 - val_accuracy: 0.5732\n",
            "Epoch 9/40\n",
            "53/53 [==============================] - 31s 591ms/step - loss: 0.7129 - accuracy: 0.4949 - val_loss: 0.6844 - val_accuracy: 0.5707\n",
            "Epoch 10/40\n",
            "53/53 [==============================] - 31s 589ms/step - loss: 0.7084 - accuracy: 0.4975 - val_loss: 0.6833 - val_accuracy: 0.5808\n",
            "Epoch 11/40\n",
            "53/53 [==============================] - 32s 603ms/step - loss: 0.7040 - accuracy: 0.5228 - val_loss: 0.6832 - val_accuracy: 0.5732\n",
            "Epoch 12/40\n",
            "53/53 [==============================] - 31s 580ms/step - loss: 0.7000 - accuracy: 0.5196 - val_loss: 0.6828 - val_accuracy: 0.5783\n",
            "Epoch 13/40\n",
            "53/53 [==============================] - 31s 591ms/step - loss: 0.6982 - accuracy: 0.5272 - val_loss: 0.6862 - val_accuracy: 0.5429\n",
            "Epoch 14/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.7003 - accuracy: 0.5278 - val_loss: 0.6816 - val_accuracy: 0.5808\n",
            "Epoch 15/40\n",
            "53/53 [==============================] - 32s 613ms/step - loss: 0.7013 - accuracy: 0.5342 - val_loss: 0.6810 - val_accuracy: 0.5884\n",
            "Epoch 16/40\n",
            "53/53 [==============================] - 32s 597ms/step - loss: 0.7005 - accuracy: 0.5215 - val_loss: 0.6855 - val_accuracy: 0.5455\n",
            "Epoch 17/40\n",
            "53/53 [==============================] - 33s 613ms/step - loss: 0.6993 - accuracy: 0.5291 - val_loss: 0.6820 - val_accuracy: 0.5934\n",
            "Epoch 18/40\n",
            "53/53 [==============================] - 31s 584ms/step - loss: 0.7066 - accuracy: 0.5266 - val_loss: 0.6808 - val_accuracy: 0.5859\n",
            "Epoch 19/40\n",
            "53/53 [==============================] - 31s 593ms/step - loss: 0.6871 - accuracy: 0.5462 - val_loss: 0.6790 - val_accuracy: 0.5934\n",
            "Epoch 20/40\n",
            "53/53 [==============================] - 32s 600ms/step - loss: 0.6912 - accuracy: 0.5405 - val_loss: 0.6764 - val_accuracy: 0.5884\n",
            "Epoch 21/40\n",
            "53/53 [==============================] - 31s 588ms/step - loss: 0.6889 - accuracy: 0.5386 - val_loss: 0.6815 - val_accuracy: 0.5808\n",
            "Epoch 22/40\n",
            "53/53 [==============================] - 31s 589ms/step - loss: 0.6963 - accuracy: 0.5348 - val_loss: 0.6764 - val_accuracy: 0.5960\n",
            "Epoch 23/40\n",
            "53/53 [==============================] - 31s 582ms/step - loss: 0.6919 - accuracy: 0.5462 - val_loss: 0.6775 - val_accuracy: 0.5934\n",
            "Epoch 24/40\n",
            "53/53 [==============================] - 32s 603ms/step - loss: 0.6867 - accuracy: 0.5392 - val_loss: 0.6762 - val_accuracy: 0.5934\n",
            "Epoch 25/40\n",
            "53/53 [==============================] - 33s 628ms/step - loss: 0.6816 - accuracy: 0.5614 - val_loss: 0.6802 - val_accuracy: 0.5783\n",
            "Epoch 26/40\n",
            "53/53 [==============================] - 32s 601ms/step - loss: 0.6880 - accuracy: 0.5715 - val_loss: 0.6773 - val_accuracy: 0.5859\n",
            "Epoch 27/40\n",
            "53/53 [==============================] - 33s 614ms/step - loss: 0.6888 - accuracy: 0.5513 - val_loss: 0.6753 - val_accuracy: 0.6010\n",
            "Epoch 28/40\n",
            "53/53 [==============================] - 31s 594ms/step - loss: 0.6898 - accuracy: 0.5437 - val_loss: 0.6792 - val_accuracy: 0.5859\n",
            "Epoch 29/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6797 - accuracy: 0.5772 - val_loss: 0.6732 - val_accuracy: 0.5985\n",
            "Epoch 30/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.6884 - accuracy: 0.5443 - val_loss: 0.6757 - val_accuracy: 0.5934\n",
            "Epoch 31/40\n",
            "53/53 [==============================] - 31s 587ms/step - loss: 0.6910 - accuracy: 0.5399 - val_loss: 0.6738 - val_accuracy: 0.6061\n",
            "Epoch 32/40\n",
            "53/53 [==============================] - 31s 586ms/step - loss: 0.6783 - accuracy: 0.5665 - val_loss: 0.6705 - val_accuracy: 0.6061\n",
            "Epoch 33/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6808 - accuracy: 0.5608 - val_loss: 0.6735 - val_accuracy: 0.6111\n",
            "Epoch 34/40\n",
            "53/53 [==============================] - 31s 592ms/step - loss: 0.6804 - accuracy: 0.5608 - val_loss: 0.6668 - val_accuracy: 0.6086\n",
            "Epoch 35/40\n",
            "53/53 [==============================] - 33s 614ms/step - loss: 0.6811 - accuracy: 0.5595 - val_loss: 0.6710 - val_accuracy: 0.6061\n",
            "Epoch 36/40\n",
            "53/53 [==============================] - 31s 579ms/step - loss: 0.6830 - accuracy: 0.5589 - val_loss: 0.6692 - val_accuracy: 0.6111\n",
            "Epoch 37/40\n",
            "53/53 [==============================] - 31s 585ms/step - loss: 0.6775 - accuracy: 0.5601 - val_loss: 0.6712 - val_accuracy: 0.6086\n",
            "Epoch 38/40\n",
            "53/53 [==============================] - 31s 590ms/step - loss: 0.6795 - accuracy: 0.5709 - val_loss: 0.6645 - val_accuracy: 0.6010\n",
            "Epoch 39/40\n",
            "53/53 [==============================] - 32s 604ms/step - loss: 0.6798 - accuracy: 0.5608 - val_loss: 0.6664 - val_accuracy: 0.6162\n",
            "Epoch 40/40\n",
            "53/53 [==============================] - 30s 572ms/step - loss: 0.6617 - accuracy: 0.6063 - val_loss: 0.6650 - val_accuracy: 0.6162\n"
          ]
        }
      ],
      "source": [
        "t=\"/content/train_l1\"\n",
        "v=\"/content/validation_l1\"\n",
        "mod_l1,hist_l1=model_l1(t,v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHg6PnC6TN7o"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrP6jbaQcV34",
        "outputId": "cba60f36-ba5f-4b88-b86c-540d6b5ff0d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "mod_l1.save(\"/content/Level1.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBCEerjuazcB"
      },
      "source": [
        "# Testing level-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jqf_M8I2Lo3Q"
      },
      "outputs": [],
      "source": [
        "test_images=[]\n",
        "test_label=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "34CGlTWO3Fv2"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/glide_100_27/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6edVDh6c3pL7"
      },
      "outputs": [],
      "source": [
        "path=\"/content/diffusion/diffusion_datasets/glide_50_27/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Pg9aSV7bM5Ck"
      },
      "outputs": [],
      "source": [
        "test_directory = \"/content/test3\"\n",
        "os.makedirs(test_directory,exist_ok=True)\n",
        "copy_images_to_directory(test_images,test_label, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g05spw5XNrGT",
        "outputId": "a5e75094-090b-4681-deb5-4d0bc1166785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3000 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = \"/content/test3\"\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSrVM4GDMEzZ",
        "outputId": "56e243bb-be16-4977-9f14-c7aaa8292a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 13s 127ms/step - loss: 0.6085 - accuracy: 0.8680\n",
            "Test Loss: 0.608487606048584\n",
            "Test Accuracy: 0.8679999709129333\n"
          ]
        }
      ],
      "source": [
        "results = mod_l1.evaluate(test_generator)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFcr19NQaBvq"
      },
      "source": [
        "# **Level-2** of the architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3ElAjJT6fq"
      },
      "source": [
        "Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CW30zPsjYKDw"
      },
      "outputs": [],
      "source": [
        "gan_images=[]\n",
        "# path=\"/content/gan/biggan/1_fake\"\n",
        "path=\"/content/gan/whichfaceisreal/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "      new_size = (224, 224)\n",
        "      img = cv2.resize(img, new_size)\n",
        "      gan_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1VWE4KGsGIA",
        "outputId": "64144f62-e587-4be2-ef8b-062b0aa0a908"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 224, 224, 3)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(gan_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "eOyf2C-HY5dV"
      },
      "outputs": [],
      "source": [
        "dm_images=[]\n",
        "# path=\"/content/diffusion/diffusion_datasets/dalle/1_fake\"\n",
        "path=\"/content/diffusion/diffusion_datasets/glide_100_10/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "      dm_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1w4n9fvtV1m",
        "outputId": "39f353b6-1755-4731-c646-a5c738b5015e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 256, 256, 3)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(dm_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "GBZd1zaOZru0"
      },
      "outputs": [],
      "source": [
        "data=[]\n",
        "labels=[]  # 0-Dm 1-Gan\n",
        "data.extend(dm_images)\n",
        "data.extend(gan_images)\n",
        "l1=[0]*len(dm_images)\n",
        "l2=[1]*len(gan_images)\n",
        "labels.extend(l1)\n",
        "labels.extend(l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2N6oTdmd_EE",
        "outputId": "ea8b0e16-9466-4054-f522-c4e0e8b89b96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Cyqxzb0eZqmN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sVgja7QAZVbv"
      },
      "outputs": [],
      "source": [
        "train_d = \"/content/train_l2\"\n",
        "val_d =  \"/content/validation_l2\"\n",
        "os.makedirs(train_d)\n",
        "os.makedirs(val_d)\n",
        "\n",
        "copy_images_to_directory(X_train, y_train, train_d)\n",
        "copy_images_to_directory(X_val, y_val, val_d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACp2AvWaINok"
      },
      "source": [
        "# **Creating Structure of Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1) Preprocessing of Data:**\n",
        "\n",
        "- Gaussian Blur.\n",
        "- Rescaling, rotation, flipping.\n",
        "- Preparing Training and Validation Generator.\n",
        "\n",
        "**2) Building Model:**\n",
        "\n",
        "- Importing weights from ResNet50 model (Pre-Trained Model).\n",
        "- **Fine Tuning:**\n",
        "  - Freezing first 100 layers (Neural network won't train back first 100 layers).\n",
        "  - Unfreezing all other layers (Weights get updated for all layers after the 100th layer).\n",
        "- Used Early Stopping to stop the training process when no significant improvement in loss is observed and retaining the best weights in each step.\n",
        "- Loss function: Binary Cross Entropy.\n",
        "\n",
        "**3) Model Structure:**\n",
        "\n",
        "- `x = base_model(inputs, training=False)`: Base_model is a pre-trained convolutional neural network (CNN) ResNet-50, used as a feature extractor.\n",
        "- `x = GlobalAveragePooling2D()(x)`: After extracting features from the base model, a global average pooling layer is applied. Global Average Pooling 2D computes the average value of each feature map across the entire spatial dimensions. This reduces the spatial dimensions to 1x1, effectively summarizing the information in each feature map.\n",
        "- `x = Dense(1024, activation='relu')(x)`: This is a fully connected (dense) layer with 1024 units and ReLU (Rectified Linear Unit) activation function. The output of the global average pooling is connected to this dense layer, introducing non-linearity and allowing the network to learn complex patterns.\n",
        "- `x = tf.keras.layers.Dropout(0.2)(x)`: Dropout is a regularization technique that helps prevent overfitting. It randomly sets a fraction of input units to zero at each update during training, which helps prevent the network from relying too much on any specific set of neurons. In this case, 20% of the units are dropped out (set to zero).\n",
        "- `outputs = Dense(1, activation='sigmoid')(x)`: The final layer is a dense layer with a single unit and a sigmoid activation function. The output is a probability between 0 and 1.\n",
        "  - 0: Real Image\n",
        "  - 1: Fake Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Learning Rate Schedule: Exponential Decay**\n",
        "\n",
        "- **Initial Learning Rate:** \\(1 \\times 10^{-4}\\) (0.0001)\n",
        "  - The starting learning rate at the beginning of training.\n",
        "\n",
        "- **Decay Steps:** 10,000\n",
        "  - After every 10,000 steps, the learning rate will be updated.\n",
        "\n",
        "- **Decay Rate:** 0.9\n",
        "  - The rate at which the learning rate will decay, multiplied at each decay step.\n",
        "\n",
        "By employing this exponentially decaying learning rate schedule, the model takes larger steps initially and gradually decreases the learning rate as training progresses, aiding in faster convergence and fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hyperparameters:**\n",
        "\n",
        "- Epochs: 40\n",
        "- Momentum: 0.9\n",
        "- Dropout Layers: 0.2\n",
        "- Optimizer: SGD (Stochastic Gradient Descent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "LmI5Wn9OIIwp"
      },
      "outputs": [],
      "source": [
        "def model_l2(train_dir,validation_dir,tune,epoc):\n",
        "    def apply_gaussian_blur(image):\n",
        "        sigma = 1.0\n",
        "        return cv2.GaussianBlur(image, (0, 0), sigma)\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        preprocessing_function=apply_gaussian_blur\n",
        "    )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(256, 256),\n",
        "        batch_size=30,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(256, 256, 3))\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "    fine_tune_at = tune\n",
        "\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "    for layer in base_model.layers[fine_tune_at:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9\n",
        "    )\n",
        "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    history=model.fit(train_generator, epochs=epoc,\n",
        "              validation_data=validation_generator,\n",
        "              callbacks = [early_stopping_callback])\n",
        "\n",
        "    return model,history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKubA3rUTyNv"
      },
      "source": [
        "# **Training** phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ9erjgOZ7hh",
        "outputId": "f4aefdd5-7233-4360-9653-64efc261e1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 400 images belonging to 2 classes.\n",
            "Number of layers in the base model:  175\n",
            "Epoch 1/40\n",
            "54/54 [==============================] - 45s 612ms/step - loss: 0.6936 - accuracy: 0.5431 - val_loss: 0.6638 - val_accuracy: 0.5150\n",
            "Epoch 2/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.7032 - accuracy: 0.5169 - val_loss: 0.6831 - val_accuracy: 0.4950\n",
            "Epoch 3/40\n",
            "54/54 [==============================] - 33s 596ms/step - loss: 0.6690 - accuracy: 0.5838 - val_loss: 0.6101 - val_accuracy: 0.7200\n",
            "Epoch 4/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.6123 - accuracy: 0.6737 - val_loss: 0.6488 - val_accuracy: 0.5550\n",
            "Epoch 5/40\n",
            "54/54 [==============================] - 33s 608ms/step - loss: 0.5849 - accuracy: 0.6662 - val_loss: 0.6206 - val_accuracy: 0.8325\n",
            "Epoch 6/40\n",
            "54/54 [==============================] - 33s 611ms/step - loss: 0.5899 - accuracy: 0.6837 - val_loss: 0.6921 - val_accuracy: 0.4975\n",
            "Epoch 7/40\n",
            "54/54 [==============================] - 32s 600ms/step - loss: 0.6570 - accuracy: 0.6256 - val_loss: 0.5553 - val_accuracy: 0.8625\n",
            "Epoch 8/40\n",
            "54/54 [==============================] - 33s 602ms/step - loss: 0.4993 - accuracy: 0.7794 - val_loss: 0.3125 - val_accuracy: 0.9125\n",
            "Epoch 9/40\n",
            "54/54 [==============================] - 33s 614ms/step - loss: 0.4651 - accuracy: 0.7881 - val_loss: 0.2573 - val_accuracy: 0.9125\n",
            "Epoch 10/40\n",
            "54/54 [==============================] - 34s 614ms/step - loss: 0.2851 - accuracy: 0.8925 - val_loss: 0.2441 - val_accuracy: 0.9150\n",
            "Epoch 11/40\n",
            "54/54 [==============================] - 33s 609ms/step - loss: 0.2260 - accuracy: 0.9156 - val_loss: 0.1171 - val_accuracy: 0.9550\n",
            "Epoch 12/40\n",
            "54/54 [==============================] - 33s 617ms/step - loss: 0.2048 - accuracy: 0.9275 - val_loss: 0.0737 - val_accuracy: 0.9775\n",
            "Epoch 13/40\n",
            "54/54 [==============================] - 32s 588ms/step - loss: 0.0944 - accuracy: 0.9669 - val_loss: 0.0447 - val_accuracy: 0.9900\n",
            "Epoch 14/40\n",
            "54/54 [==============================] - 34s 613ms/step - loss: 0.1051 - accuracy: 0.9594 - val_loss: 0.1146 - val_accuracy: 0.9525\n",
            "Epoch 15/40\n",
            "54/54 [==============================] - 33s 600ms/step - loss: 0.1038 - accuracy: 0.9631 - val_loss: 0.0319 - val_accuracy: 0.9875\n",
            "Epoch 16/40\n",
            "54/54 [==============================] - 33s 604ms/step - loss: 0.0752 - accuracy: 0.9744 - val_loss: 0.0237 - val_accuracy: 0.9950\n",
            "Epoch 17/40\n",
            "54/54 [==============================] - 32s 589ms/step - loss: 0.0345 - accuracy: 0.9869 - val_loss: 0.0257 - val_accuracy: 0.9925\n",
            "Epoch 18/40\n",
            "54/54 [==============================] - 34s 624ms/step - loss: 0.0506 - accuracy: 0.9794 - val_loss: 0.0403 - val_accuracy: 0.9850\n",
            "Epoch 19/40\n",
            "54/54 [==============================] - 32s 596ms/step - loss: 0.0527 - accuracy: 0.9831 - val_loss: 0.0288 - val_accuracy: 0.9900\n",
            "Epoch 20/40\n",
            "54/54 [==============================] - 32s 598ms/step - loss: 0.0263 - accuracy: 0.9931 - val_loss: 0.0680 - val_accuracy: 0.9625\n",
            "Epoch 21/40\n",
            "54/54 [==============================] - 33s 612ms/step - loss: 0.0301 - accuracy: 0.9875 - val_loss: 0.0144 - val_accuracy: 0.9925\n",
            "Epoch 22/40\n",
            "54/54 [==============================] - 35s 634ms/step - loss: 0.0207 - accuracy: 0.9925 - val_loss: 0.0404 - val_accuracy: 0.9800\n",
            "Epoch 23/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.0238 - val_accuracy: 0.9950\n",
            "Epoch 24/40\n",
            "54/54 [==============================] - 33s 606ms/step - loss: 0.0110 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9975\n",
            "Epoch 25/40\n",
            "54/54 [==============================] - 32s 589ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0216 - val_accuracy: 0.9900\n",
            "Epoch 26/40\n",
            "54/54 [==============================] - 32s 599ms/step - loss: 0.0081 - accuracy: 0.9981 - val_loss: 0.0097 - val_accuracy: 0.9925\n",
            "Epoch 27/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.0281 - val_accuracy: 0.9875\n",
            "Epoch 28/40\n",
            "54/54 [==============================] - 33s 602ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 0.0118 - val_accuracy: 0.9950\n",
            "Epoch 29/40\n",
            "54/54 [==============================] - 32s 592ms/step - loss: 0.0099 - accuracy: 0.9956 - val_loss: 0.0103 - val_accuracy: 0.9925\n",
            "Epoch 30/40\n",
            "54/54 [==============================] - 34s 633ms/step - loss: 0.0092 - accuracy: 0.9975 - val_loss: 0.0102 - val_accuracy: 0.9975\n",
            "Epoch 31/40\n",
            "54/54 [==============================] - 32s 596ms/step - loss: 0.0085 - accuracy: 0.9950 - val_loss: 0.0229 - val_accuracy: 0.9900\n",
            "Epoch 32/40\n",
            "54/54 [==============================] - 33s 615ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0222 - val_accuracy: 0.9875\n",
            "Epoch 33/40\n",
            "54/54 [==============================] - 32s 591ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.0099 - val_accuracy: 0.9950\n",
            "Epoch 34/40\n",
            "54/54 [==============================] - 32s 594ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0198 - val_accuracy: 0.9925\n",
            "Epoch 35/40\n",
            "54/54 [==============================] - 32s 593ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9950\n",
            "Epoch 36/40\n",
            "54/54 [==============================] - 34s 627ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 0.9950\n",
            "Epoch 37/40\n",
            "54/54 [==============================] - 34s 621ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9950\n",
            "Epoch 38/40\n",
            "54/54 [==============================] - 33s 603ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0153 - val_accuracy: 0.9950\n",
            "Epoch 39/40\n",
            "54/54 [==============================] - 33s 618ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0131 - val_accuracy: 0.9925\n",
            "Epoch 40/40\n",
            "54/54 [==============================] - 33s 597ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9950\n"
          ]
        }
      ],
      "source": [
        "t=\"/content/train_l2\"\n",
        "v=\"/content/validation_l2\"\n",
        "mod_l2,hist_l2=model_l2(t,v,100,40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3YKECopTuHO"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or6YM7VWf8We",
        "outputId": "de3dea92-1eba-4de4-eccd-2dcfc00c9287"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "mod_l2.save(\"/content/Level2_updated.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IycL-pZ_e2hB"
      },
      "source": [
        "# Testing Level-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dso9j11meyv-"
      },
      "outputs": [],
      "source": [
        "test_images=[]\n",
        "test_label=[]\n",
        "path=\"/content/diffusion/diffusion_datasets/guided/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7LMh5puZbHl5"
      },
      "outputs": [],
      "source": [
        "path=\"/content/gan/stargan/1_fake\"\n",
        "l=os.listdir(path)\n",
        "for i in l:\n",
        "    img=np.array(Image.open(os.path.join(path,i)))\n",
        "    if len(np.shape(img))==3:\n",
        "        test_images.append(img)\n",
        "        test_label.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "8-HDAbNVfekS"
      },
      "outputs": [],
      "source": [
        "test_directory = \"/content/test5\"\n",
        "os.makedirs(test_directory,exist_ok=True)\n",
        "copy_images_to_directory(test_images,test_label, test_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgDuvHd_fm6Z",
        "outputId": "37c802cd-5146-4124-e29b-9e5a25cdf7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2999 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_dir = \"/content/test5\"\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=30,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqOhNe_8fox3",
        "outputId": "2367f833-a274-4193-fe62-7d18550e0c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 12s 123ms/step - loss: 0.2682 - accuracy: 0.9300\n",
            "Test Loss: 0.2682065963745117\n",
            "Test Accuracy: 0.9299766421318054\n"
          ]
        }
      ],
      "source": [
        "results = mod_l2.evaluate(test_generator)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test Accuracy:\", results[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
